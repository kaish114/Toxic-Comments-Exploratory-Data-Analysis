{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Some times people say nasty things over the internet because they feel anonymous there. So lets filter out some hate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective:\n",
    "I am going to perform Exploratory data analysis for toxic comment classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset:\n",
    "The dataset here is from wiki corpus dataset which was rated by human raters for toxicity.\n",
    "The corpus contains 63M comments from discussions relating to user pages and articles dating from 2004-2015. \n",
    "\n",
    "Different platforms/sites can have different standards for their toxic screening process. Hence the comments are tagged in the following five categories\n",
    "* toxic\n",
    "* severe_toxic\n",
    "* obscene\n",
    "* threat\n",
    "* insult\n",
    "* identity_hate\n",
    "\n",
    "[Dataset](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we need to import required libraries\n",
    "#basic libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "#misc\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "#statistics\n",
    "from scipy.misc import imread\n",
    "from scipy import sparse\n",
    "import scipy.stats as ss\n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec \n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud ,STOPWORDS\n",
    "from PIL import Image\n",
    "import matplotlib_venn as venn\n",
    "\n",
    "#natural language processing\n",
    "import string\n",
    "import re    #for regex\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tweet tokenizer does not split at apostophes which is what we want\n",
    "from nltk.tokenize import TweetTokenizer   \n",
    "\n",
    "\n",
    "#FeatureEngineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#noraml settings\n",
    "start_time=time.time()\n",
    "color = sns.color_palette()\n",
    "sns.set_style(\"dark\")\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "tokenizer=TweetTokenizer()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "train=pd.read_csv(\"train.csv\")\n",
    "test=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
